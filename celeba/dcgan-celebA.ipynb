{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import math\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "latent_dim = 100\n",
    "img_size = 64\n",
    "IMAGE_PATH = \"./celeba/img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('sample_images'):\n",
    "    os.mkdir('sample_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = img_size // 16\n",
    "        self.rel = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.l1 = nn.Sequential(nn.Linear(100, 128 * self.init_size ** 2))\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.deconv1_bn = nn.BatchNorm2d(64)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(32)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 16, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(16)\n",
    "        self.deconv4 = nn.ConvTranspose2d(16, 3, 4, 2, 1)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # x = F.relu(self.deconv1(input))\n",
    "        x = self.l1(input)\n",
    "        x = x.view(x.shape[0], 128, self.init_size, self.init_size)\n",
    "        x = self.rel(self.deconv1_bn(self.deconv1(x)))\n",
    "        x = self.rel(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = self.rel(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.tanh(self.deconv4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            *discriminator_block(3, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.fc_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "#         print(img.shape)\n",
    "        x = self.convs(img)\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "#         print(x.shape)\n",
    "        x = self.fc_layer(x)\n",
    "#         print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (rel): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (l1): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=2048, bias=True)\n",
       "  )\n",
       "  (deconv1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv3_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (convs): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(IMAGE_PATH, transform)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!\n",
      "[Epoch 5/5] [Batch 0/3165] [D loss: 0.608303] [G loss: 0.962178]\n",
      "[Epoch 5/5] [Batch 100/3165] [D loss: 0.565683] [G loss: 0.769831]\n",
      "[Epoch 5/5] [Batch 200/3165] [D loss: 0.568141] [G loss: 0.720198]\n",
      "[Epoch 5/5] [Batch 300/3165] [D loss: 0.583818] [G loss: 1.069882]\n",
      "[Epoch 5/5] [Batch 400/3165] [D loss: 0.540312] [G loss: 0.816347]\n",
      "[Epoch 5/5] [Batch 500/3165] [D loss: 0.379157] [G loss: 1.450890]\n",
      "[Epoch 5/5] [Batch 600/3165] [D loss: 0.457385] [G loss: 0.756448]\n",
      "[Epoch 5/5] [Batch 700/3165] [D loss: 0.641378] [G loss: 0.999967]\n",
      "[Epoch 5/5] [Batch 800/3165] [D loss: 0.717636] [G loss: 1.011828]\n",
      "[Epoch 5/5] [Batch 900/3165] [D loss: 0.570377] [G loss: 0.907937]\n",
      "[Epoch 5/5] [Batch 1000/3165] [D loss: 0.664378] [G loss: 0.572132]\n",
      "[Epoch 5/5] [Batch 1100/3165] [D loss: 0.675641] [G loss: 0.846205]\n",
      "[Epoch 5/5] [Batch 1200/3165] [D loss: 0.543508] [G loss: 1.029174]\n",
      "[Epoch 5/5] [Batch 1300/3165] [D loss: 0.772108] [G loss: 0.781758]\n",
      "[Epoch 5/5] [Batch 1400/3165] [D loss: 0.646080] [G loss: 1.470167]\n",
      "[Epoch 5/5] [Batch 1500/3165] [D loss: 0.539048] [G loss: 0.709786]\n",
      "[Epoch 5/5] [Batch 1600/3165] [D loss: 0.578150] [G loss: 1.052227]\n",
      "[Epoch 5/5] [Batch 1700/3165] [D loss: 0.588156] [G loss: 1.042385]\n",
      "[Epoch 5/5] [Batch 1800/3165] [D loss: 0.647090] [G loss: 0.607302]\n",
      "[Epoch 5/5] [Batch 1900/3165] [D loss: 0.578545] [G loss: 0.746953]\n",
      "[Epoch 5/5] [Batch 2000/3165] [D loss: 0.494515] [G loss: 1.198535]\n",
      "[Epoch 5/5] [Batch 2100/3165] [D loss: 0.694123] [G loss: 0.793959]\n",
      "[Epoch 5/5] [Batch 2200/3165] [D loss: 0.592703] [G loss: 0.926573]\n",
      "[Epoch 5/5] [Batch 2300/3165] [D loss: 0.363504] [G loss: 1.471488]\n",
      "[Epoch 5/5] [Batch 2400/3165] [D loss: 0.498624] [G loss: 0.842778]\n",
      "[Epoch 5/5] [Batch 2500/3165] [D loss: 0.651312] [G loss: 1.114058]\n",
      "[Epoch 5/5] [Batch 2600/3165] [D loss: 0.599440] [G loss: 1.223926]\n",
      "[Epoch 5/5] [Batch 2700/3165] [D loss: 0.579877] [G loss: 0.978339]\n",
      "[Epoch 5/5] [Batch 2800/3165] [D loss: 0.537669] [G loss: 0.687760]\n",
      "[Epoch 5/5] [Batch 2900/3165] [D loss: 0.772431] [G loss: 1.105400]\n",
      "[Epoch 5/5] [Batch 3000/3165] [D loss: 0.605045] [G loss: 1.058471]\n",
      "[Epoch 5/5] [Batch 3100/3165] [D loss: 0.598328] [G loss: 1.012309]\n",
      "[Epoch 6/5] [Batch 0/3165] [D loss: 0.707174] [G loss: 0.835395]\n",
      "[Epoch 6/5] [Batch 100/3165] [D loss: 0.432907] [G loss: 1.127706]\n",
      "[Epoch 6/5] [Batch 200/3165] [D loss: 0.624043] [G loss: 1.003677]\n",
      "[Epoch 6/5] [Batch 300/3165] [D loss: 0.754987] [G loss: 0.737538]\n",
      "[Epoch 6/5] [Batch 400/3165] [D loss: 0.503132] [G loss: 1.232391]\n",
      "[Epoch 6/5] [Batch 500/3165] [D loss: 0.835463] [G loss: 0.750609]\n",
      "[Epoch 6/5] [Batch 600/3165] [D loss: 0.599703] [G loss: 0.899238]\n",
      "[Epoch 6/5] [Batch 700/3165] [D loss: 0.583539] [G loss: 1.182394]\n",
      "[Epoch 6/5] [Batch 800/3165] [D loss: 0.531975] [G loss: 0.997247]\n",
      "[Epoch 6/5] [Batch 900/3165] [D loss: 1.071569] [G loss: 0.569121]\n",
      "[Epoch 6/5] [Batch 1000/3165] [D loss: 0.659578] [G loss: 1.274784]\n",
      "[Epoch 6/5] [Batch 1100/3165] [D loss: 0.777854] [G loss: 0.959852]\n",
      "[Epoch 6/5] [Batch 1200/3165] [D loss: 0.673198] [G loss: 1.001158]\n",
      "[Epoch 6/5] [Batch 1300/3165] [D loss: 0.460964] [G loss: 0.853736]\n",
      "[Epoch 6/5] [Batch 1400/3165] [D loss: 0.610134] [G loss: 1.040185]\n",
      "[Epoch 6/5] [Batch 1500/3165] [D loss: 0.678721] [G loss: 1.160781]\n",
      "[Epoch 6/5] [Batch 1600/3165] [D loss: 0.445192] [G loss: 0.936171]\n",
      "[Epoch 6/5] [Batch 1700/3165] [D loss: 0.628608] [G loss: 1.227526]\n",
      "[Epoch 6/5] [Batch 1800/3165] [D loss: 0.539214] [G loss: 0.915247]\n",
      "[Epoch 6/5] [Batch 1900/3165] [D loss: 0.537525] [G loss: 1.257845]\n",
      "[Epoch 6/5] [Batch 2000/3165] [D loss: 0.712830] [G loss: 0.991346]\n",
      "[Epoch 6/5] [Batch 2100/3165] [D loss: 0.642495] [G loss: 0.978217]\n",
      "[Epoch 6/5] [Batch 2200/3165] [D loss: 0.552995] [G loss: 1.003095]\n",
      "[Epoch 6/5] [Batch 2300/3165] [D loss: 1.016151] [G loss: 0.400902]\n",
      "[Epoch 6/5] [Batch 2400/3165] [D loss: 0.528630] [G loss: 0.942134]\n",
      "[Epoch 6/5] [Batch 2500/3165] [D loss: 0.557819] [G loss: 1.151732]\n",
      "[Epoch 6/5] [Batch 2600/3165] [D loss: 0.486711] [G loss: 0.917928]\n",
      "[Epoch 6/5] [Batch 2700/3165] [D loss: 0.452670] [G loss: 1.335814]\n",
      "[Epoch 6/5] [Batch 2800/3165] [D loss: 0.559684] [G loss: 0.952830]\n",
      "[Epoch 6/5] [Batch 2900/3165] [D loss: 0.556357] [G loss: 1.125506]\n",
      "[Epoch 6/5] [Batch 3000/3165] [D loss: 0.501608] [G loss: 1.015134]\n",
      "[Epoch 6/5] [Batch 3100/3165] [D loss: 0.791762] [G loss: 1.141661]\n",
      "[Epoch 7/5] [Batch 0/3165] [D loss: 0.561819] [G loss: 0.838197]\n",
      "[Epoch 7/5] [Batch 100/3165] [D loss: 0.723625] [G loss: 0.993563]\n",
      "[Epoch 7/5] [Batch 200/3165] [D loss: 0.802270] [G loss: 0.545440]\n",
      "[Epoch 7/5] [Batch 300/3165] [D loss: 0.572972] [G loss: 1.310086]\n",
      "[Epoch 7/5] [Batch 400/3165] [D loss: 0.507909] [G loss: 1.031734]\n",
      "[Epoch 7/5] [Batch 500/3165] [D loss: 0.658570] [G loss: 1.184103]\n",
      "[Epoch 7/5] [Batch 600/3165] [D loss: 0.542815] [G loss: 0.751381]\n",
      "[Epoch 7/5] [Batch 700/3165] [D loss: 0.556314] [G loss: 1.112197]\n",
      "[Epoch 7/5] [Batch 800/3165] [D loss: 0.525484] [G loss: 0.951148]\n",
      "[Epoch 7/5] [Batch 900/3165] [D loss: 0.828103] [G loss: 1.352240]\n",
      "[Epoch 7/5] [Batch 1000/3165] [D loss: 0.484210] [G loss: 1.131162]\n",
      "[Epoch 7/5] [Batch 1100/3165] [D loss: 0.649699] [G loss: 0.928211]\n",
      "[Epoch 7/5] [Batch 1200/3165] [D loss: 0.543846] [G loss: 1.045375]\n",
      "[Epoch 7/5] [Batch 1300/3165] [D loss: 0.416948] [G loss: 0.841017]\n",
      "[Epoch 7/5] [Batch 1400/3165] [D loss: 0.577320] [G loss: 0.910077]\n",
      "[Epoch 7/5] [Batch 1500/3165] [D loss: 0.553239] [G loss: 0.755886]\n",
      "[Epoch 7/5] [Batch 1600/3165] [D loss: 0.398691] [G loss: 1.019984]\n",
      "[Epoch 7/5] [Batch 1700/3165] [D loss: 0.744942] [G loss: 1.195276]\n",
      "[Epoch 7/5] [Batch 1800/3165] [D loss: 0.467584] [G loss: 1.190024]\n",
      "[Epoch 7/5] [Batch 1900/3165] [D loss: 0.576448] [G loss: 0.911945]\n",
      "[Epoch 7/5] [Batch 2000/3165] [D loss: 0.562503] [G loss: 1.075796]\n",
      "[Epoch 7/5] [Batch 2100/3165] [D loss: 0.654259] [G loss: 0.904243]\n",
      "[Epoch 7/5] [Batch 2200/3165] [D loss: 0.523058] [G loss: 0.866674]\n",
      "[Epoch 7/5] [Batch 2300/3165] [D loss: 0.650457] [G loss: 0.990830]\n",
      "[Epoch 7/5] [Batch 2400/3165] [D loss: 0.602270] [G loss: 0.800831]\n",
      "[Epoch 7/5] [Batch 2500/3165] [D loss: 0.507353] [G loss: 1.288181]\n",
      "[Epoch 7/5] [Batch 2600/3165] [D loss: 0.607507] [G loss: 0.898630]\n",
      "[Epoch 7/5] [Batch 2700/3165] [D loss: 0.943708] [G loss: 0.557100]\n",
      "[Epoch 7/5] [Batch 2800/3165] [D loss: 0.588337] [G loss: 0.684948]\n",
      "[Epoch 7/5] [Batch 2900/3165] [D loss: 0.652512] [G loss: 1.007149]\n",
      "[Epoch 7/5] [Batch 3000/3165] [D loss: 0.566515] [G loss: 0.645256]\n",
      "[Epoch 7/5] [Batch 3100/3165] [D loss: 0.489257] [G loss: 0.985615]\n",
      "[Epoch 8/5] [Batch 0/3165] [D loss: 0.514277] [G loss: 1.057454]\n",
      "[Epoch 8/5] [Batch 100/3165] [D loss: 0.504449] [G loss: 0.966914]\n",
      "[Epoch 8/5] [Batch 200/3165] [D loss: 0.678524] [G loss: 0.992416]\n",
      "[Epoch 8/5] [Batch 300/3165] [D loss: 0.543966] [G loss: 0.845081]\n",
      "[Epoch 8/5] [Batch 400/3165] [D loss: 0.597137] [G loss: 0.883973]\n",
      "[Epoch 8/5] [Batch 500/3165] [D loss: 0.533310] [G loss: 1.182836]\n",
      "[Epoch 8/5] [Batch 600/3165] [D loss: 0.660844] [G loss: 0.743668]\n",
      "[Epoch 8/5] [Batch 700/3165] [D loss: 0.633386] [G loss: 1.043172]\n",
      "[Epoch 8/5] [Batch 800/3165] [D loss: 0.489151] [G loss: 0.980022]\n",
      "[Epoch 8/5] [Batch 900/3165] [D loss: 0.704212] [G loss: 0.828899]\n",
      "[Epoch 8/5] [Batch 1000/3165] [D loss: 0.561125] [G loss: 0.969447]\n",
      "[Epoch 8/5] [Batch 1100/3165] [D loss: 0.545220] [G loss: 1.048825]\n",
      "[Epoch 8/5] [Batch 1200/3165] [D loss: 0.601247] [G loss: 1.407804]\n",
      "[Epoch 8/5] [Batch 1300/3165] [D loss: 0.639763] [G loss: 0.646483]\n",
      "[Epoch 8/5] [Batch 1400/3165] [D loss: 0.669004] [G loss: 0.928142]\n",
      "[Epoch 8/5] [Batch 1500/3165] [D loss: 0.649453] [G loss: 0.801280]\n",
      "[Epoch 8/5] [Batch 1600/3165] [D loss: 0.844245] [G loss: 0.697528]\n",
      "[Epoch 8/5] [Batch 1700/3165] [D loss: 0.659720] [G loss: 0.834721]\n",
      "[Epoch 8/5] [Batch 1800/3165] [D loss: 0.625183] [G loss: 0.880930]\n",
      "[Epoch 8/5] [Batch 1900/3165] [D loss: 0.640723] [G loss: 0.875141]\n",
      "[Epoch 8/5] [Batch 2000/3165] [D loss: 0.485083] [G loss: 0.745822]\n",
      "[Epoch 8/5] [Batch 2100/3165] [D loss: 0.776932] [G loss: 0.878669]\n",
      "[Epoch 8/5] [Batch 2200/3165] [D loss: 0.688582] [G loss: 1.056520]\n",
      "[Epoch 8/5] [Batch 2300/3165] [D loss: 0.688941] [G loss: 0.812509]\n",
      "[Epoch 8/5] [Batch 2400/3165] [D loss: 0.475298] [G loss: 0.966228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/5] [Batch 2500/3165] [D loss: 0.722125] [G loss: 0.899239]\n",
      "[Epoch 8/5] [Batch 2600/3165] [D loss: 0.586959] [G loss: 1.169443]\n",
      "[Epoch 8/5] [Batch 2700/3165] [D loss: 0.676356] [G loss: 0.880139]\n",
      "[Epoch 8/5] [Batch 2800/3165] [D loss: 0.569396] [G loss: 0.675996]\n",
      "[Epoch 8/5] [Batch 2900/3165] [D loss: 0.482604] [G loss: 0.720669]\n",
      "[Epoch 8/5] [Batch 3000/3165] [D loss: 0.593776] [G loss: 0.885963]\n",
      "[Epoch 8/5] [Batch 3100/3165] [D loss: 0.516833] [G loss: 1.009062]\n",
      "[Epoch 9/5] [Batch 0/3165] [D loss: 0.584358] [G loss: 1.004774]\n",
      "[Epoch 9/5] [Batch 100/3165] [D loss: 0.806777] [G loss: 0.696592]\n",
      "[Epoch 9/5] [Batch 200/3165] [D loss: 0.534036] [G loss: 1.008659]\n",
      "[Epoch 9/5] [Batch 300/3165] [D loss: 0.540458] [G loss: 0.996163]\n",
      "[Epoch 9/5] [Batch 400/3165] [D loss: 0.653046] [G loss: 0.796747]\n",
      "[Epoch 9/5] [Batch 500/3165] [D loss: 0.768449] [G loss: 1.059432]\n",
      "[Epoch 9/5] [Batch 600/3165] [D loss: 0.647552] [G loss: 1.207552]\n",
      "[Epoch 9/5] [Batch 700/3165] [D loss: 0.643697] [G loss: 1.045180]\n",
      "[Epoch 9/5] [Batch 800/3165] [D loss: 0.692816] [G loss: 0.565315]\n",
      "[Epoch 9/5] [Batch 900/3165] [D loss: 0.541579] [G loss: 0.721231]\n",
      "[Epoch 9/5] [Batch 1000/3165] [D loss: 0.769104] [G loss: 0.738539]\n",
      "[Epoch 9/5] [Batch 1100/3165] [D loss: 0.466009] [G loss: 0.920140]\n",
      "[Epoch 9/5] [Batch 1200/3165] [D loss: 0.546860] [G loss: 0.956379]\n",
      "[Epoch 9/5] [Batch 1300/3165] [D loss: 0.589151] [G loss: 0.820307]\n",
      "[Epoch 9/5] [Batch 1400/3165] [D loss: 0.642920] [G loss: 1.245464]\n",
      "[Epoch 9/5] [Batch 1500/3165] [D loss: 0.532985] [G loss: 1.014697]\n",
      "[Epoch 9/5] [Batch 1600/3165] [D loss: 0.589718] [G loss: 1.128325]\n",
      "[Epoch 9/5] [Batch 1700/3165] [D loss: 0.711524] [G loss: 0.679269]\n",
      "[Epoch 9/5] [Batch 1800/3165] [D loss: 0.614464] [G loss: 0.896150]\n",
      "[Epoch 9/5] [Batch 1900/3165] [D loss: 0.589257] [G loss: 0.755072]\n",
      "[Epoch 9/5] [Batch 2000/3165] [D loss: 0.557872] [G loss: 0.780055]\n",
      "[Epoch 9/5] [Batch 2100/3165] [D loss: 0.586641] [G loss: 0.868930]\n",
      "[Epoch 9/5] [Batch 2200/3165] [D loss: 0.577407] [G loss: 0.906665]\n",
      "[Epoch 9/5] [Batch 2300/3165] [D loss: 0.646857] [G loss: 1.013372]\n",
      "[Epoch 9/5] [Batch 2400/3165] [D loss: 0.474977] [G loss: 1.148808]\n",
      "[Epoch 9/5] [Batch 2500/3165] [D loss: 0.751581] [G loss: 1.002711]\n",
      "[Epoch 9/5] [Batch 2600/3165] [D loss: 0.646263] [G loss: 0.559397]\n",
      "[Epoch 9/5] [Batch 2700/3165] [D loss: 0.899327] [G loss: 1.050546]\n",
      "[Epoch 9/5] [Batch 2800/3165] [D loss: 0.586120] [G loss: 0.908581]\n",
      "[Epoch 9/5] [Batch 2900/3165] [D loss: 0.683115] [G loss: 1.241158]\n",
      "[Epoch 9/5] [Batch 3000/3165] [D loss: 0.658593] [G loss: 0.814531]\n",
      "[Epoch 9/5] [Batch 3100/3165] [D loss: 0.769084] [G loss: 0.669070]\n",
      "[Epoch 10/5] [Batch 0/3165] [D loss: 0.789405] [G loss: 1.328657]\n",
      "[Epoch 10/5] [Batch 100/3165] [D loss: 0.636193] [G loss: 0.999101]\n",
      "[Epoch 10/5] [Batch 200/3165] [D loss: 0.575857] [G loss: 0.612343]\n",
      "[Epoch 10/5] [Batch 300/3165] [D loss: 0.625506] [G loss: 1.141341]\n",
      "[Epoch 10/5] [Batch 400/3165] [D loss: 0.535864] [G loss: 0.962901]\n",
      "[Epoch 10/5] [Batch 500/3165] [D loss: 0.630833] [G loss: 0.733240]\n",
      "[Epoch 10/5] [Batch 600/3165] [D loss: 0.940202] [G loss: 0.707048]\n",
      "[Epoch 10/5] [Batch 700/3165] [D loss: 0.596907] [G loss: 0.898890]\n",
      "[Epoch 10/5] [Batch 800/3165] [D loss: 0.746323] [G loss: 1.064196]\n",
      "[Epoch 10/5] [Batch 900/3165] [D loss: 0.615631] [G loss: 0.805094]\n",
      "[Epoch 10/5] [Batch 1000/3165] [D loss: 0.690292] [G loss: 0.919011]\n",
      "[Epoch 10/5] [Batch 1100/3165] [D loss: 0.662568] [G loss: 1.026622]\n",
      "[Epoch 10/5] [Batch 1200/3165] [D loss: 0.536770] [G loss: 1.305749]\n",
      "[Epoch 10/5] [Batch 1300/3165] [D loss: 0.526504] [G loss: 0.940402]\n",
      "[Epoch 10/5] [Batch 1400/3165] [D loss: 0.618725] [G loss: 0.709974]\n",
      "[Epoch 10/5] [Batch 1500/3165] [D loss: 0.675541] [G loss: 0.704015]\n",
      "[Epoch 10/5] [Batch 1600/3165] [D loss: 0.545459] [G loss: 0.916846]\n",
      "[Epoch 10/5] [Batch 1700/3165] [D loss: 0.596028] [G loss: 0.900381]\n",
      "[Epoch 10/5] [Batch 1800/3165] [D loss: 0.716982] [G loss: 1.019051]\n",
      "[Epoch 10/5] [Batch 1900/3165] [D loss: 0.713988] [G loss: 0.913524]\n",
      "[Epoch 10/5] [Batch 2000/3165] [D loss: 0.600999] [G loss: 1.009329]\n",
      "[Epoch 10/5] [Batch 2100/3165] [D loss: 0.691491] [G loss: 1.111751]\n",
      "[Epoch 10/5] [Batch 2200/3165] [D loss: 0.607614] [G loss: 0.686816]\n",
      "[Epoch 10/5] [Batch 2300/3165] [D loss: 0.645442] [G loss: 0.995231]\n",
      "[Epoch 10/5] [Batch 2400/3165] [D loss: 0.493991] [G loss: 1.200149]\n",
      "[Epoch 10/5] [Batch 2500/3165] [D loss: 0.753676] [G loss: 1.003779]\n",
      "[Epoch 10/5] [Batch 2600/3165] [D loss: 0.695457] [G loss: 0.622445]\n",
      "[Epoch 10/5] [Batch 2700/3165] [D loss: 0.615129] [G loss: 0.922750]\n",
      "[Epoch 10/5] [Batch 2800/3165] [D loss: 0.747846] [G loss: 0.923036]\n",
      "[Epoch 10/5] [Batch 2900/3165] [D loss: 0.637618] [G loss: 1.244242]\n",
      "[Epoch 10/5] [Batch 3000/3165] [D loss: 0.566544] [G loss: 1.107940]\n",
      "[Epoch 10/5] [Batch 3100/3165] [D loss: 0.666211] [G loss: 0.543073]\n",
      "[Epoch 11/5] [Batch 0/3165] [D loss: 0.530594] [G loss: 1.083270]\n",
      "[Epoch 11/5] [Batch 100/3165] [D loss: 0.536395] [G loss: 0.956915]\n",
      "[Epoch 11/5] [Batch 200/3165] [D loss: 0.676335] [G loss: 0.799027]\n",
      "[Epoch 11/5] [Batch 300/3165] [D loss: 0.736808] [G loss: 0.753656]\n",
      "[Epoch 11/5] [Batch 400/3165] [D loss: 0.524034] [G loss: 0.789961]\n",
      "[Epoch 11/5] [Batch 500/3165] [D loss: 0.670244] [G loss: 0.704357]\n",
      "[Epoch 11/5] [Batch 600/3165] [D loss: 0.583307] [G loss: 0.929309]\n",
      "[Epoch 11/5] [Batch 700/3165] [D loss: 0.713384] [G loss: 1.031601]\n",
      "[Epoch 11/5] [Batch 800/3165] [D loss: 0.655456] [G loss: 0.658770]\n",
      "[Epoch 11/5] [Batch 900/3165] [D loss: 0.638910] [G loss: 0.811677]\n",
      "[Epoch 11/5] [Batch 1000/3165] [D loss: 0.546985] [G loss: 0.985901]\n",
      "[Epoch 11/5] [Batch 1100/3165] [D loss: 0.671029] [G loss: 1.399191]\n",
      "[Epoch 11/5] [Batch 1200/3165] [D loss: 0.598861] [G loss: 1.179786]\n",
      "[Epoch 11/5] [Batch 1300/3165] [D loss: 0.457362] [G loss: 1.021071]\n",
      "[Epoch 11/5] [Batch 1400/3165] [D loss: 0.460766] [G loss: 1.000675]\n",
      "[Epoch 11/5] [Batch 1500/3165] [D loss: 0.663202] [G loss: 0.801071]\n",
      "[Epoch 11/5] [Batch 1600/3165] [D loss: 0.587791] [G loss: 0.888293]\n",
      "[Epoch 11/5] [Batch 1700/3165] [D loss: 0.746501] [G loss: 0.930706]\n",
      "[Epoch 11/5] [Batch 1800/3165] [D loss: 0.598125] [G loss: 1.000262]\n",
      "[Epoch 11/5] [Batch 1900/3165] [D loss: 0.610959] [G loss: 0.804273]\n",
      "[Epoch 11/5] [Batch 2000/3165] [D loss: 0.536201] [G loss: 1.064250]\n",
      "[Epoch 11/5] [Batch 2100/3165] [D loss: 0.519917] [G loss: 0.912206]\n",
      "[Epoch 11/5] [Batch 2200/3165] [D loss: 0.626598] [G loss: 0.760270]\n",
      "[Epoch 11/5] [Batch 2300/3165] [D loss: 0.587777] [G loss: 0.990398]\n",
      "[Epoch 11/5] [Batch 2400/3165] [D loss: 0.621262] [G loss: 0.894116]\n",
      "[Epoch 11/5] [Batch 2500/3165] [D loss: 0.499801] [G loss: 0.725566]\n",
      "[Epoch 11/5] [Batch 2600/3165] [D loss: 0.528404] [G loss: 1.178999]\n",
      "[Epoch 11/5] [Batch 2700/3165] [D loss: 0.618935] [G loss: 0.945108]\n",
      "[Epoch 11/5] [Batch 2800/3165] [D loss: 0.592965] [G loss: 0.960300]\n",
      "[Epoch 11/5] [Batch 2900/3165] [D loss: 0.599668] [G loss: 0.922420]\n",
      "[Epoch 11/5] [Batch 3000/3165] [D loss: 0.729911] [G loss: 0.862322]\n",
      "[Epoch 11/5] [Batch 3100/3165] [D loss: 0.441818] [G loss: 0.956426]\n",
      "[Epoch 12/5] [Batch 0/3165] [D loss: 0.710836] [G loss: 0.872970]\n",
      "[Epoch 12/5] [Batch 100/3165] [D loss: 0.724583] [G loss: 0.824811]\n",
      "[Epoch 12/5] [Batch 200/3165] [D loss: 0.468884] [G loss: 0.764073]\n",
      "[Epoch 12/5] [Batch 300/3165] [D loss: 0.562723] [G loss: 1.020652]\n",
      "[Epoch 12/5] [Batch 400/3165] [D loss: 0.472943] [G loss: 1.327181]\n",
      "[Epoch 12/5] [Batch 500/3165] [D loss: 0.670203] [G loss: 1.039034]\n",
      "[Epoch 12/5] [Batch 600/3165] [D loss: 0.597247] [G loss: 1.062563]\n",
      "[Epoch 12/5] [Batch 700/3165] [D loss: 0.666301] [G loss: 1.176353]\n",
      "[Epoch 12/5] [Batch 800/3165] [D loss: 0.481430] [G loss: 0.961577]\n",
      "[Epoch 12/5] [Batch 900/3165] [D loss: 0.391454] [G loss: 0.802989]\n",
      "[Epoch 12/5] [Batch 1000/3165] [D loss: 0.698114] [G loss: 1.362155]\n",
      "[Epoch 12/5] [Batch 1100/3165] [D loss: 0.599203] [G loss: 0.805984]\n",
      "[Epoch 12/5] [Batch 1200/3165] [D loss: 0.575535] [G loss: 0.589758]\n",
      "[Epoch 12/5] [Batch 1300/3165] [D loss: 0.588558] [G loss: 0.988891]\n",
      "[Epoch 12/5] [Batch 1400/3165] [D loss: 0.481919] [G loss: 0.694855]\n",
      "[Epoch 12/5] [Batch 1500/3165] [D loss: 0.530762] [G loss: 0.978011]\n",
      "[Epoch 12/5] [Batch 1600/3165] [D loss: 0.512774] [G loss: 1.256568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/5] [Batch 1700/3165] [D loss: 0.746452] [G loss: 0.658376]\n",
      "[Epoch 12/5] [Batch 1800/3165] [D loss: 0.789595] [G loss: 1.179258]\n",
      "[Epoch 12/5] [Batch 1900/3165] [D loss: 0.545387] [G loss: 1.483832]\n",
      "[Epoch 12/5] [Batch 2000/3165] [D loss: 0.464230] [G loss: 0.859230]\n",
      "[Epoch 12/5] [Batch 2100/3165] [D loss: 0.582333] [G loss: 0.848294]\n",
      "[Epoch 12/5] [Batch 2200/3165] [D loss: 0.603481] [G loss: 0.948896]\n",
      "[Epoch 12/5] [Batch 2300/3165] [D loss: 0.481259] [G loss: 1.148590]\n",
      "[Epoch 12/5] [Batch 2400/3165] [D loss: 0.790206] [G loss: 0.805694]\n",
      "[Epoch 12/5] [Batch 2500/3165] [D loss: 0.605837] [G loss: 0.984990]\n",
      "[Epoch 12/5] [Batch 2600/3165] [D loss: 0.564756] [G loss: 0.741347]\n",
      "[Epoch 12/5] [Batch 2700/3165] [D loss: 0.476353] [G loss: 0.745212]\n",
      "[Epoch 12/5] [Batch 2800/3165] [D loss: 0.559584] [G loss: 1.565217]\n",
      "[Epoch 12/5] [Batch 2900/3165] [D loss: 0.480121] [G loss: 0.779332]\n",
      "[Epoch 12/5] [Batch 3000/3165] [D loss: 0.862112] [G loss: 0.656593]\n",
      "[Epoch 12/5] [Batch 3100/3165] [D loss: 0.577320] [G loss: 1.098569]\n",
      "[Epoch 13/5] [Batch 0/3165] [D loss: 0.562747] [G loss: 0.857271]\n",
      "[Epoch 13/5] [Batch 100/3165] [D loss: 0.518842] [G loss: 1.038172]\n",
      "[Epoch 13/5] [Batch 200/3165] [D loss: 0.436654] [G loss: 1.416850]\n",
      "[Epoch 13/5] [Batch 300/3165] [D loss: 0.499527] [G loss: 0.667481]\n",
      "[Epoch 13/5] [Batch 400/3165] [D loss: 0.460646] [G loss: 0.821483]\n",
      "[Epoch 13/5] [Batch 500/3165] [D loss: 0.601178] [G loss: 1.225386]\n",
      "[Epoch 13/5] [Batch 600/3165] [D loss: 0.602888] [G loss: 0.689419]\n",
      "[Epoch 13/5] [Batch 700/3165] [D loss: 0.536633] [G loss: 0.644700]\n",
      "[Epoch 13/5] [Batch 800/3165] [D loss: 0.461278] [G loss: 0.702801]\n",
      "[Epoch 13/5] [Batch 900/3165] [D loss: 0.516000] [G loss: 1.078987]\n",
      "[Epoch 13/5] [Batch 1000/3165] [D loss: 0.559301] [G loss: 1.287053]\n",
      "[Epoch 13/5] [Batch 1100/3165] [D loss: 0.556956] [G loss: 0.943328]\n",
      "[Epoch 13/5] [Batch 1200/3165] [D loss: 0.589502] [G loss: 0.989674]\n",
      "[Epoch 13/5] [Batch 1300/3165] [D loss: 0.668535] [G loss: 1.189135]\n",
      "[Epoch 13/5] [Batch 1400/3165] [D loss: 0.705044] [G loss: 0.975097]\n",
      "[Epoch 13/5] [Batch 1500/3165] [D loss: 0.597904] [G loss: 1.587652]\n",
      "[Epoch 13/5] [Batch 1600/3165] [D loss: 0.524332] [G loss: 1.331899]\n",
      "[Epoch 13/5] [Batch 1700/3165] [D loss: 0.486791] [G loss: 0.814524]\n",
      "[Epoch 13/5] [Batch 1800/3165] [D loss: 0.544657] [G loss: 0.820972]\n",
      "[Epoch 13/5] [Batch 1900/3165] [D loss: 0.612675] [G loss: 0.985995]\n",
      "[Epoch 13/5] [Batch 2000/3165] [D loss: 0.595191] [G loss: 0.710630]\n",
      "[Epoch 13/5] [Batch 2100/3165] [D loss: 0.489298] [G loss: 1.110634]\n",
      "[Epoch 13/5] [Batch 2200/3165] [D loss: 0.579101] [G loss: 1.141866]\n",
      "[Epoch 13/5] [Batch 2300/3165] [D loss: 0.512579] [G loss: 1.042209]\n",
      "[Epoch 13/5] [Batch 2400/3165] [D loss: 0.872233] [G loss: 0.375765]\n",
      "[Epoch 13/5] [Batch 2500/3165] [D loss: 0.593487] [G loss: 0.754458]\n",
      "[Epoch 13/5] [Batch 2600/3165] [D loss: 0.667412] [G loss: 0.768589]\n",
      "[Epoch 13/5] [Batch 2700/3165] [D loss: 0.508784] [G loss: 1.161320]\n",
      "[Epoch 13/5] [Batch 2800/3165] [D loss: 0.820963] [G loss: 1.244743]\n",
      "[Epoch 13/5] [Batch 2900/3165] [D loss: 0.491374] [G loss: 1.109288]\n",
      "[Epoch 13/5] [Batch 3000/3165] [D loss: 0.434155] [G loss: 1.257604]\n",
      "[Epoch 13/5] [Batch 3100/3165] [D loss: 0.497459] [G loss: 1.075423]\n",
      "[Epoch 14/5] [Batch 0/3165] [D loss: 0.844213] [G loss: 0.708310]\n",
      "[Epoch 14/5] [Batch 100/3165] [D loss: 0.687267] [G loss: 0.571388]\n",
      "[Epoch 14/5] [Batch 200/3165] [D loss: 0.555787] [G loss: 0.925262]\n",
      "[Epoch 14/5] [Batch 300/3165] [D loss: 0.655231] [G loss: 1.266730]\n",
      "[Epoch 14/5] [Batch 400/3165] [D loss: 0.443630] [G loss: 0.851463]\n",
      "[Epoch 14/5] [Batch 500/3165] [D loss: 0.658938] [G loss: 1.431352]\n",
      "[Epoch 14/5] [Batch 600/3165] [D loss: 0.535015] [G loss: 0.783532]\n",
      "[Epoch 14/5] [Batch 700/3165] [D loss: 0.639464] [G loss: 0.793855]\n",
      "[Epoch 14/5] [Batch 800/3165] [D loss: 0.592706] [G loss: 0.937067]\n",
      "[Epoch 14/5] [Batch 900/3165] [D loss: 0.551423] [G loss: 0.605749]\n",
      "[Epoch 14/5] [Batch 1000/3165] [D loss: 0.747503] [G loss: 1.178150]\n",
      "[Epoch 14/5] [Batch 1100/3165] [D loss: 0.573082] [G loss: 1.088964]\n",
      "[Epoch 14/5] [Batch 1200/3165] [D loss: 0.625792] [G loss: 1.020890]\n",
      "[Epoch 14/5] [Batch 1300/3165] [D loss: 0.362318] [G loss: 0.900573]\n",
      "[Epoch 14/5] [Batch 1400/3165] [D loss: 0.670448] [G loss: 1.154101]\n",
      "[Epoch 14/5] [Batch 1500/3165] [D loss: 0.507458] [G loss: 0.897770]\n",
      "[Epoch 14/5] [Batch 1600/3165] [D loss: 0.523495] [G loss: 0.940635]\n",
      "[Epoch 14/5] [Batch 1700/3165] [D loss: 0.708445] [G loss: 1.249949]\n",
      "[Epoch 14/5] [Batch 1800/3165] [D loss: 0.680501] [G loss: 1.482315]\n",
      "[Epoch 14/5] [Batch 1900/3165] [D loss: 0.467824] [G loss: 1.792529]\n",
      "[Epoch 14/5] [Batch 2000/3165] [D loss: 0.521966] [G loss: 1.406991]\n",
      "[Epoch 14/5] [Batch 2100/3165] [D loss: 0.599649] [G loss: 1.071816]\n",
      "[Epoch 14/5] [Batch 2200/3165] [D loss: 0.608040] [G loss: 1.117293]\n",
      "[Epoch 14/5] [Batch 2300/3165] [D loss: 0.479979] [G loss: 1.250256]\n",
      "[Epoch 14/5] [Batch 2400/3165] [D loss: 0.512949] [G loss: 0.825310]\n",
      "[Epoch 14/5] [Batch 2500/3165] [D loss: 0.397133] [G loss: 0.883652]\n",
      "[Epoch 14/5] [Batch 2600/3165] [D loss: 0.682565] [G loss: 0.612950]\n",
      "[Epoch 14/5] [Batch 2700/3165] [D loss: 0.719525] [G loss: 1.234853]\n",
      "[Epoch 14/5] [Batch 2800/3165] [D loss: 0.471507] [G loss: 0.762831]\n",
      "[Epoch 14/5] [Batch 2900/3165] [D loss: 0.711215] [G loss: 0.351093]\n",
      "[Epoch 14/5] [Batch 3000/3165] [D loss: 0.510708] [G loss: 0.820685]\n",
      "[Epoch 14/5] [Batch 3100/3165] [D loss: 0.365217] [G loss: 1.506767]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1076.4832208156586"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('training start!')\n",
    "start_time = time.time()\n",
    "for epoch in range(5,15):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        #  Train Generator\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, epochs, i, len(data_loader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "        batches_done = epoch * len(data_loader) + i\n",
    "    save_image(gen_imgs.data[:16], \"sample_images/%d.png\" % epoch, nrow=4, normalize=True)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for epoch in range(epochs):\n",
    "    img_name = 'sample_images/' + str(epoch) + '.png'\n",
    "    images.append(imageio.imread(img_name))\n",
    "imageio.mimsave('sample_images/animation.gif', images, fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
