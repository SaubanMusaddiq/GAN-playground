{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import math\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 64\n",
    "latent_dim = 100\n",
    "img_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('sample_images'):\n",
    "    os.mkdir('sample_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    # initializers\n",
    "    def __init__(self, d=16):\n",
    "        super(Generator, self).__init__()\n",
    "        self.init_size = img_size // 16\n",
    "        self.rel = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.l1 = nn.Sequential(nn.Linear(100, 128 * self.init_size ** 2))\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.deconv1_bn = nn.BatchNorm2d(64)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, 4, 2, 1)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(32)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 16, 4, 2, 1)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(16)\n",
    "        self.deconv4 = nn.ConvTranspose2d(16, 1, 4, 2, 1)\n",
    "\n",
    "    # weight_init\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    # forward method\n",
    "    def forward(self, input):\n",
    "        # x = F.relu(self.deconv1(input))\n",
    "        x = self.l1(input)\n",
    "        x = x.view(x.shape[0], 128, self.init_size, self.init_size)\n",
    "        x = self.rel(self.deconv1_bn(self.deconv1(x)))\n",
    "        x = self.rel(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = self.rel(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.tanh(self.deconv4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            *discriminator_block(1, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.fc_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.convs(img)\n",
    "        x = out.view(x.shape[0], -1)\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (rel): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (l1): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "  )\n",
       "  (deconv1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv3_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv4): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (convs): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sauban/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data2', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sauban/.local/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!\n",
      "[Epoch 0/25] [Batch 0/938] [D loss: 0.693135] [G loss: 0.688521]\n",
      "[Epoch 0/25] [Batch 100/938] [D loss: 0.078718] [G loss: 2.480105]\n",
      "[Epoch 0/25] [Batch 200/938] [D loss: 0.274360] [G loss: 2.432323]\n",
      "[Epoch 0/25] [Batch 300/938] [D loss: 0.563196] [G loss: 1.030362]\n",
      "[Epoch 0/25] [Batch 400/938] [D loss: 0.443058] [G loss: 1.651758]\n",
      "[Epoch 0/25] [Batch 500/938] [D loss: 0.437042] [G loss: 1.696185]\n",
      "[Epoch 0/25] [Batch 600/938] [D loss: 0.725458] [G loss: 1.040406]\n",
      "[Epoch 0/25] [Batch 700/938] [D loss: 0.493642] [G loss: 1.081107]\n",
      "[Epoch 0/25] [Batch 800/938] [D loss: 0.392997] [G loss: 1.797325]\n",
      "[Epoch 0/25] [Batch 900/938] [D loss: 0.452959] [G loss: 0.989500]\n",
      "[Epoch 1/25] [Batch 0/938] [D loss: 0.533845] [G loss: 1.087710]\n",
      "[Epoch 1/25] [Batch 100/938] [D loss: 0.514282] [G loss: 1.125285]\n",
      "[Epoch 1/25] [Batch 200/938] [D loss: 0.706578] [G loss: 1.006861]\n",
      "[Epoch 1/25] [Batch 300/938] [D loss: 0.682207] [G loss: 1.055139]\n",
      "[Epoch 1/25] [Batch 400/938] [D loss: 0.465723] [G loss: 0.988935]\n",
      "[Epoch 1/25] [Batch 500/938] [D loss: 0.520353] [G loss: 1.393801]\n",
      "[Epoch 1/25] [Batch 600/938] [D loss: 0.661896] [G loss: 0.972883]\n",
      "[Epoch 1/25] [Batch 700/938] [D loss: 0.619385] [G loss: 0.761887]\n",
      "[Epoch 1/25] [Batch 800/938] [D loss: 0.636377] [G loss: 1.059105]\n",
      "[Epoch 1/25] [Batch 900/938] [D loss: 0.576001] [G loss: 0.975192]\n",
      "[Epoch 2/25] [Batch 0/938] [D loss: 0.879268] [G loss: 0.797239]\n",
      "[Epoch 2/25] [Batch 100/938] [D loss: 0.506895] [G loss: 1.209581]\n",
      "[Epoch 2/25] [Batch 200/938] [D loss: 0.698347] [G loss: 0.936872]\n",
      "[Epoch 2/25] [Batch 300/938] [D loss: 0.703965] [G loss: 0.748864]\n",
      "[Epoch 2/25] [Batch 400/938] [D loss: 0.680300] [G loss: 0.884568]\n",
      "[Epoch 2/25] [Batch 500/938] [D loss: 0.471748] [G loss: 1.419650]\n",
      "[Epoch 2/25] [Batch 600/938] [D loss: 0.315246] [G loss: 1.029365]\n",
      "[Epoch 2/25] [Batch 700/938] [D loss: 0.503037] [G loss: 1.079056]\n",
      "[Epoch 2/25] [Batch 800/938] [D loss: 0.460122] [G loss: 1.019299]\n",
      "[Epoch 2/25] [Batch 900/938] [D loss: 0.598505] [G loss: 0.762533]\n",
      "[Epoch 3/25] [Batch 0/938] [D loss: 0.503269] [G loss: 0.868632]\n",
      "[Epoch 3/25] [Batch 100/938] [D loss: 0.660200] [G loss: 0.933536]\n",
      "[Epoch 3/25] [Batch 200/938] [D loss: 0.793553] [G loss: 0.815858]\n",
      "[Epoch 3/25] [Batch 300/938] [D loss: 0.699524] [G loss: 0.856334]\n",
      "[Epoch 3/25] [Batch 400/938] [D loss: 0.568635] [G loss: 1.047346]\n",
      "[Epoch 3/25] [Batch 500/938] [D loss: 0.612986] [G loss: 0.765222]\n",
      "[Epoch 3/25] [Batch 600/938] [D loss: 0.606898] [G loss: 0.903006]\n",
      "[Epoch 3/25] [Batch 700/938] [D loss: 0.705668] [G loss: 0.650480]\n",
      "[Epoch 3/25] [Batch 800/938] [D loss: 0.444483] [G loss: 1.176848]\n",
      "[Epoch 3/25] [Batch 900/938] [D loss: 0.609907] [G loss: 1.037150]\n",
      "[Epoch 4/25] [Batch 0/938] [D loss: 0.585271] [G loss: 1.035360]\n",
      "[Epoch 4/25] [Batch 100/938] [D loss: 0.653804] [G loss: 0.928498]\n",
      "[Epoch 4/25] [Batch 200/938] [D loss: 0.661347] [G loss: 0.941340]\n",
      "[Epoch 4/25] [Batch 300/938] [D loss: 0.532211] [G loss: 0.966727]\n",
      "[Epoch 4/25] [Batch 400/938] [D loss: 0.562374] [G loss: 1.176155]\n",
      "[Epoch 4/25] [Batch 500/938] [D loss: 0.642589] [G loss: 1.104706]\n",
      "[Epoch 4/25] [Batch 600/938] [D loss: 0.479186] [G loss: 1.144821]\n",
      "[Epoch 4/25] [Batch 700/938] [D loss: 0.541718] [G loss: 0.946586]\n",
      "[Epoch 4/25] [Batch 800/938] [D loss: 0.740438] [G loss: 0.790903]\n",
      "[Epoch 4/25] [Batch 900/938] [D loss: 0.579953] [G loss: 0.628906]\n",
      "[Epoch 5/25] [Batch 0/938] [D loss: 0.763962] [G loss: 0.789406]\n",
      "[Epoch 5/25] [Batch 100/938] [D loss: 0.611254] [G loss: 0.812908]\n",
      "[Epoch 5/25] [Batch 200/938] [D loss: 0.686092] [G loss: 0.579519]\n",
      "[Epoch 5/25] [Batch 300/938] [D loss: 0.719504] [G loss: 0.756069]\n",
      "[Epoch 5/25] [Batch 400/938] [D loss: 0.669079] [G loss: 0.852516]\n",
      "[Epoch 5/25] [Batch 500/938] [D loss: 0.541376] [G loss: 0.865497]\n",
      "[Epoch 5/25] [Batch 600/938] [D loss: 0.737103] [G loss: 0.821277]\n",
      "[Epoch 5/25] [Batch 700/938] [D loss: 0.673264] [G loss: 0.794198]\n",
      "[Epoch 5/25] [Batch 800/938] [D loss: 0.536727] [G loss: 0.870536]\n",
      "[Epoch 5/25] [Batch 900/938] [D loss: 0.579570] [G loss: 0.808459]\n",
      "[Epoch 6/25] [Batch 0/938] [D loss: 0.597619] [G loss: 1.306077]\n",
      "[Epoch 6/25] [Batch 100/938] [D loss: 0.761922] [G loss: 0.783028]\n",
      "[Epoch 6/25] [Batch 200/938] [D loss: 0.681887] [G loss: 0.887310]\n",
      "[Epoch 6/25] [Batch 300/938] [D loss: 0.730849] [G loss: 0.886357]\n",
      "[Epoch 6/25] [Batch 400/938] [D loss: 0.744215] [G loss: 0.955367]\n",
      "[Epoch 6/25] [Batch 500/938] [D loss: 0.712637] [G loss: 0.810259]\n",
      "[Epoch 6/25] [Batch 600/938] [D loss: 0.732519] [G loss: 0.628265]\n",
      "[Epoch 6/25] [Batch 700/938] [D loss: 0.799195] [G loss: 0.916959]\n",
      "[Epoch 6/25] [Batch 800/938] [D loss: 0.727681] [G loss: 0.863078]\n",
      "[Epoch 6/25] [Batch 900/938] [D loss: 0.727918] [G loss: 0.758807]\n",
      "[Epoch 7/25] [Batch 0/938] [D loss: 0.717723] [G loss: 0.825152]\n",
      "[Epoch 7/25] [Batch 100/938] [D loss: 0.705782] [G loss: 0.668356]\n",
      "[Epoch 7/25] [Batch 200/938] [D loss: 0.607528] [G loss: 0.882137]\n",
      "[Epoch 7/25] [Batch 300/938] [D loss: 0.605843] [G loss: 0.763564]\n",
      "[Epoch 7/25] [Batch 400/938] [D loss: 0.723154] [G loss: 0.671529]\n",
      "[Epoch 7/25] [Batch 500/938] [D loss: 0.667128] [G loss: 0.717196]\n",
      "[Epoch 7/25] [Batch 600/938] [D loss: 0.659037] [G loss: 0.794714]\n",
      "[Epoch 7/25] [Batch 700/938] [D loss: 0.795808] [G loss: 0.554147]\n",
      "[Epoch 7/25] [Batch 800/938] [D loss: 0.679063] [G loss: 0.670897]\n",
      "[Epoch 7/25] [Batch 900/938] [D loss: 0.666678] [G loss: 0.768409]\n",
      "[Epoch 8/25] [Batch 0/938] [D loss: 0.615255] [G loss: 0.690211]\n",
      "[Epoch 8/25] [Batch 100/938] [D loss: 0.740273] [G loss: 0.789167]\n",
      "[Epoch 8/25] [Batch 200/938] [D loss: 0.657241] [G loss: 0.757622]\n",
      "[Epoch 8/25] [Batch 300/938] [D loss: 0.679933] [G loss: 0.705092]\n",
      "[Epoch 8/25] [Batch 400/938] [D loss: 0.707416] [G loss: 0.739184]\n",
      "[Epoch 8/25] [Batch 500/938] [D loss: 0.672371] [G loss: 0.716817]\n",
      "[Epoch 8/25] [Batch 600/938] [D loss: 0.692517] [G loss: 0.645999]\n",
      "[Epoch 8/25] [Batch 700/938] [D loss: 0.659467] [G loss: 0.692319]\n",
      "[Epoch 8/25] [Batch 800/938] [D loss: 0.655680] [G loss: 0.660506]\n",
      "[Epoch 8/25] [Batch 900/938] [D loss: 0.704943] [G loss: 0.601229]\n",
      "[Epoch 9/25] [Batch 0/938] [D loss: 0.696750] [G loss: 0.656850]\n",
      "[Epoch 9/25] [Batch 100/938] [D loss: 0.701878] [G loss: 0.581479]\n",
      "[Epoch 9/25] [Batch 200/938] [D loss: 0.672029] [G loss: 0.783086]\n",
      "[Epoch 9/25] [Batch 300/938] [D loss: 0.660628] [G loss: 0.713164]\n",
      "[Epoch 9/25] [Batch 400/938] [D loss: 0.668072] [G loss: 0.650817]\n",
      "[Epoch 9/25] [Batch 500/938] [D loss: 0.621809] [G loss: 0.799258]\n",
      "[Epoch 9/25] [Batch 600/938] [D loss: 0.634965] [G loss: 0.645893]\n",
      "[Epoch 9/25] [Batch 700/938] [D loss: 0.737121] [G loss: 0.681586]\n",
      "[Epoch 9/25] [Batch 800/938] [D loss: 0.733409] [G loss: 0.659217]\n",
      "[Epoch 9/25] [Batch 900/938] [D loss: 0.688035] [G loss: 0.515980]\n",
      "[Epoch 10/25] [Batch 0/938] [D loss: 0.679060] [G loss: 0.721923]\n",
      "[Epoch 10/25] [Batch 100/938] [D loss: 0.703724] [G loss: 0.666989]\n",
      "[Epoch 10/25] [Batch 200/938] [D loss: 0.677319] [G loss: 0.807938]\n",
      "[Epoch 10/25] [Batch 300/938] [D loss: 0.738960] [G loss: 0.761325]\n",
      "[Epoch 10/25] [Batch 400/938] [D loss: 0.613999] [G loss: 0.876028]\n",
      "[Epoch 10/25] [Batch 500/938] [D loss: 0.536478] [G loss: 0.816596]\n",
      "[Epoch 10/25] [Batch 600/938] [D loss: 0.688177] [G loss: 0.687028]\n",
      "[Epoch 10/25] [Batch 700/938] [D loss: 0.675662] [G loss: 0.738470]\n",
      "[Epoch 10/25] [Batch 800/938] [D loss: 0.701847] [G loss: 0.645841]\n",
      "[Epoch 10/25] [Batch 900/938] [D loss: 0.660137] [G loss: 0.722910]\n",
      "[Epoch 11/25] [Batch 0/938] [D loss: 0.647052] [G loss: 0.816663]\n",
      "[Epoch 11/25] [Batch 100/938] [D loss: 0.683379] [G loss: 0.513066]\n",
      "[Epoch 11/25] [Batch 200/938] [D loss: 0.637666] [G loss: 0.747682]\n",
      "[Epoch 11/25] [Batch 300/938] [D loss: 0.703435] [G loss: 0.699801]\n",
      "[Epoch 11/25] [Batch 400/938] [D loss: 0.669084] [G loss: 0.916106]\n",
      "[Epoch 11/25] [Batch 500/938] [D loss: 0.585368] [G loss: 0.922429]\n",
      "[Epoch 11/25] [Batch 600/938] [D loss: 0.659540] [G loss: 0.804406]\n",
      "[Epoch 11/25] [Batch 700/938] [D loss: 0.683429] [G loss: 0.825087]\n",
      "[Epoch 11/25] [Batch 800/938] [D loss: 0.605804] [G loss: 0.835818]\n",
      "[Epoch 11/25] [Batch 900/938] [D loss: 0.621230] [G loss: 0.914400]\n",
      "[Epoch 12/25] [Batch 0/938] [D loss: 0.619828] [G loss: 0.892974]\n",
      "[Epoch 12/25] [Batch 100/938] [D loss: 0.648612] [G loss: 0.786208]\n",
      "[Epoch 12/25] [Batch 200/938] [D loss: 0.605054] [G loss: 0.845406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/25] [Batch 300/938] [D loss: 0.606479] [G loss: 0.766586]\n",
      "[Epoch 12/25] [Batch 400/938] [D loss: 0.619509] [G loss: 0.827589]\n",
      "[Epoch 12/25] [Batch 500/938] [D loss: 0.561211] [G loss: 0.745492]\n",
      "[Epoch 12/25] [Batch 600/938] [D loss: 0.616167] [G loss: 0.823430]\n",
      "[Epoch 12/25] [Batch 700/938] [D loss: 0.572836] [G loss: 0.661386]\n",
      "[Epoch 12/25] [Batch 800/938] [D loss: 0.760903] [G loss: 0.795779]\n",
      "[Epoch 12/25] [Batch 900/938] [D loss: 0.611744] [G loss: 0.673179]\n",
      "[Epoch 13/25] [Batch 0/938] [D loss: 0.693281] [G loss: 0.638996]\n",
      "[Epoch 13/25] [Batch 100/938] [D loss: 0.715883] [G loss: 0.802268]\n",
      "[Epoch 13/25] [Batch 200/938] [D loss: 0.638263] [G loss: 0.751245]\n",
      "[Epoch 13/25] [Batch 300/938] [D loss: 0.684516] [G loss: 1.104545]\n",
      "[Epoch 13/25] [Batch 400/938] [D loss: 0.708819] [G loss: 0.627218]\n",
      "[Epoch 13/25] [Batch 500/938] [D loss: 0.579914] [G loss: 0.789038]\n",
      "[Epoch 13/25] [Batch 600/938] [D loss: 0.753861] [G loss: 0.617646]\n",
      "[Epoch 13/25] [Batch 700/938] [D loss: 0.548656] [G loss: 0.863444]\n",
      "[Epoch 13/25] [Batch 800/938] [D loss: 0.592194] [G loss: 1.262110]\n",
      "[Epoch 13/25] [Batch 900/938] [D loss: 0.657529] [G loss: 0.907876]\n",
      "[Epoch 14/25] [Batch 0/938] [D loss: 0.645048] [G loss: 1.199408]\n",
      "[Epoch 14/25] [Batch 100/938] [D loss: 0.879941] [G loss: 0.578411]\n",
      "[Epoch 14/25] [Batch 200/938] [D loss: 0.605013] [G loss: 0.862318]\n",
      "[Epoch 14/25] [Batch 300/938] [D loss: 0.650248] [G loss: 1.146177]\n",
      "[Epoch 14/25] [Batch 400/938] [D loss: 0.661524] [G loss: 0.835854]\n",
      "[Epoch 14/25] [Batch 500/938] [D loss: 0.636784] [G loss: 0.823757]\n",
      "[Epoch 14/25] [Batch 600/938] [D loss: 0.677580] [G loss: 0.807211]\n",
      "[Epoch 14/25] [Batch 700/938] [D loss: 0.626237] [G loss: 0.850262]\n",
      "[Epoch 14/25] [Batch 800/938] [D loss: 0.691968] [G loss: 1.011999]\n",
      "[Epoch 14/25] [Batch 900/938] [D loss: 0.475913] [G loss: 1.053129]\n",
      "[Epoch 15/25] [Batch 0/938] [D loss: 0.685340] [G loss: 1.050184]\n",
      "[Epoch 15/25] [Batch 100/938] [D loss: 0.778011] [G loss: 0.914250]\n",
      "[Epoch 15/25] [Batch 200/938] [D loss: 0.662569] [G loss: 0.722475]\n",
      "[Epoch 15/25] [Batch 300/938] [D loss: 0.631009] [G loss: 1.057637]\n",
      "[Epoch 15/25] [Batch 400/938] [D loss: 0.463020] [G loss: 1.242657]\n",
      "[Epoch 15/25] [Batch 500/938] [D loss: 0.613405] [G loss: 0.848793]\n",
      "[Epoch 15/25] [Batch 600/938] [D loss: 0.782799] [G loss: 1.134424]\n",
      "[Epoch 15/25] [Batch 700/938] [D loss: 0.689315] [G loss: 0.625942]\n",
      "[Epoch 15/25] [Batch 800/938] [D loss: 0.533236] [G loss: 0.736336]\n",
      "[Epoch 15/25] [Batch 900/938] [D loss: 0.658555] [G loss: 0.710999]\n",
      "[Epoch 16/25] [Batch 0/938] [D loss: 0.673414] [G loss: 0.681912]\n",
      "[Epoch 16/25] [Batch 100/938] [D loss: 0.598954] [G loss: 0.825212]\n",
      "[Epoch 16/25] [Batch 200/938] [D loss: 0.669352] [G loss: 0.858769]\n",
      "[Epoch 16/25] [Batch 300/938] [D loss: 0.545683] [G loss: 0.815821]\n",
      "[Epoch 16/25] [Batch 400/938] [D loss: 0.487360] [G loss: 0.919338]\n",
      "[Epoch 16/25] [Batch 500/938] [D loss: 0.420481] [G loss: 1.120085]\n",
      "[Epoch 16/25] [Batch 600/938] [D loss: 0.511070] [G loss: 0.856974]\n",
      "[Epoch 16/25] [Batch 700/938] [D loss: 0.484373] [G loss: 0.765788]\n",
      "[Epoch 16/25] [Batch 800/938] [D loss: 0.544363] [G loss: 0.760788]\n",
      "[Epoch 16/25] [Batch 900/938] [D loss: 0.597155] [G loss: 0.616854]\n",
      "[Epoch 17/25] [Batch 0/938] [D loss: 0.638067] [G loss: 1.127264]\n",
      "[Epoch 17/25] [Batch 100/938] [D loss: 0.705300] [G loss: 1.152547]\n",
      "[Epoch 17/25] [Batch 200/938] [D loss: 0.674001] [G loss: 0.688025]\n",
      "[Epoch 17/25] [Batch 300/938] [D loss: 0.648363] [G loss: 0.627236]\n",
      "[Epoch 17/25] [Batch 400/938] [D loss: 0.568064] [G loss: 0.757300]\n",
      "[Epoch 17/25] [Batch 500/938] [D loss: 0.523033] [G loss: 0.541066]\n",
      "[Epoch 17/25] [Batch 600/938] [D loss: 0.598665] [G loss: 1.258962]\n",
      "[Epoch 17/25] [Batch 700/938] [D loss: 0.715402] [G loss: 1.134703]\n",
      "[Epoch 17/25] [Batch 800/938] [D loss: 0.808068] [G loss: 0.454406]\n",
      "[Epoch 17/25] [Batch 900/938] [D loss: 0.512886] [G loss: 1.527771]\n",
      "[Epoch 18/25] [Batch 0/938] [D loss: 0.708126] [G loss: 0.672004]\n",
      "[Epoch 18/25] [Batch 100/938] [D loss: 0.582492] [G loss: 0.637720]\n",
      "[Epoch 18/25] [Batch 200/938] [D loss: 0.402950] [G loss: 1.048157]\n",
      "[Epoch 18/25] [Batch 300/938] [D loss: 0.550982] [G loss: 0.875746]\n",
      "[Epoch 18/25] [Batch 400/938] [D loss: 0.776908] [G loss: 1.114344]\n",
      "[Epoch 18/25] [Batch 500/938] [D loss: 0.639321] [G loss: 0.980522]\n",
      "[Epoch 18/25] [Batch 600/938] [D loss: 0.453503] [G loss: 0.992479]\n",
      "[Epoch 18/25] [Batch 700/938] [D loss: 0.592810] [G loss: 1.200256]\n",
      "[Epoch 18/25] [Batch 800/938] [D loss: 0.608381] [G loss: 0.456934]\n",
      "[Epoch 18/25] [Batch 900/938] [D loss: 0.558280] [G loss: 1.035791]\n",
      "[Epoch 19/25] [Batch 0/938] [D loss: 0.457420] [G loss: 1.447134]\n",
      "[Epoch 19/25] [Batch 100/938] [D loss: 0.749675] [G loss: 0.721560]\n",
      "[Epoch 19/25] [Batch 200/938] [D loss: 0.505090] [G loss: 0.828097]\n",
      "[Epoch 19/25] [Batch 300/938] [D loss: 0.613492] [G loss: 1.187945]\n",
      "[Epoch 19/25] [Batch 400/938] [D loss: 0.572723] [G loss: 0.823708]\n",
      "[Epoch 19/25] [Batch 500/938] [D loss: 0.656711] [G loss: 1.220373]\n",
      "[Epoch 19/25] [Batch 600/938] [D loss: 0.730249] [G loss: 1.026088]\n",
      "[Epoch 19/25] [Batch 700/938] [D loss: 0.473187] [G loss: 0.650587]\n",
      "[Epoch 19/25] [Batch 800/938] [D loss: 0.568541] [G loss: 0.830853]\n",
      "[Epoch 19/25] [Batch 900/938] [D loss: 0.623514] [G loss: 1.177366]\n",
      "[Epoch 20/25] [Batch 0/938] [D loss: 0.661165] [G loss: 0.994813]\n",
      "[Epoch 20/25] [Batch 100/938] [D loss: 0.493236] [G loss: 1.099242]\n",
      "[Epoch 20/25] [Batch 200/938] [D loss: 0.576017] [G loss: 1.252434]\n",
      "[Epoch 20/25] [Batch 300/938] [D loss: 0.626934] [G loss: 1.196834]\n",
      "[Epoch 20/25] [Batch 400/938] [D loss: 0.330737] [G loss: 0.792303]\n",
      "[Epoch 20/25] [Batch 500/938] [D loss: 0.377337] [G loss: 1.232823]\n",
      "[Epoch 20/25] [Batch 600/938] [D loss: 0.327857] [G loss: 1.594480]\n",
      "[Epoch 20/25] [Batch 700/938] [D loss: 0.474077] [G loss: 0.902689]\n",
      "[Epoch 20/25] [Batch 800/938] [D loss: 0.694124] [G loss: 0.738364]\n",
      "[Epoch 20/25] [Batch 900/938] [D loss: 0.570416] [G loss: 0.739158]\n",
      "[Epoch 21/25] [Batch 0/938] [D loss: 0.829339] [G loss: 0.886874]\n",
      "[Epoch 21/25] [Batch 100/938] [D loss: 0.468517] [G loss: 1.081901]\n",
      "[Epoch 21/25] [Batch 200/938] [D loss: 0.502462] [G loss: 1.907827]\n",
      "[Epoch 21/25] [Batch 300/938] [D loss: 0.655854] [G loss: 0.931545]\n",
      "[Epoch 21/25] [Batch 400/938] [D loss: 0.562436] [G loss: 1.206461]\n",
      "[Epoch 21/25] [Batch 500/938] [D loss: 0.458559] [G loss: 0.830981]\n",
      "[Epoch 21/25] [Batch 600/938] [D loss: 0.546463] [G loss: 1.142221]\n",
      "[Epoch 21/25] [Batch 700/938] [D loss: 0.334967] [G loss: 0.984333]\n",
      "[Epoch 21/25] [Batch 800/938] [D loss: 0.632019] [G loss: 0.840041]\n",
      "[Epoch 21/25] [Batch 900/938] [D loss: 0.341955] [G loss: 0.899221]\n",
      "[Epoch 22/25] [Batch 0/938] [D loss: 0.663088] [G loss: 1.242822]\n",
      "[Epoch 22/25] [Batch 100/938] [D loss: 0.417250] [G loss: 1.753463]\n",
      "[Epoch 22/25] [Batch 200/938] [D loss: 0.521184] [G loss: 0.851064]\n",
      "[Epoch 22/25] [Batch 300/938] [D loss: 0.483454] [G loss: 1.695146]\n",
      "[Epoch 22/25] [Batch 400/938] [D loss: 0.613628] [G loss: 0.845611]\n",
      "[Epoch 22/25] [Batch 500/938] [D loss: 0.532377] [G loss: 0.616192]\n",
      "[Epoch 22/25] [Batch 600/938] [D loss: 0.414779] [G loss: 1.304990]\n",
      "[Epoch 22/25] [Batch 700/938] [D loss: 0.492294] [G loss: 0.947322]\n",
      "[Epoch 22/25] [Batch 800/938] [D loss: 0.404841] [G loss: 1.170239]\n",
      "[Epoch 22/25] [Batch 900/938] [D loss: 0.560368] [G loss: 1.182707]\n",
      "[Epoch 23/25] [Batch 0/938] [D loss: 0.505270] [G loss: 1.731862]\n",
      "[Epoch 23/25] [Batch 100/938] [D loss: 0.469896] [G loss: 1.339159]\n",
      "[Epoch 23/25] [Batch 200/938] [D loss: 0.516114] [G loss: 0.847929]\n",
      "[Epoch 23/25] [Batch 300/938] [D loss: 0.513776] [G loss: 0.727500]\n",
      "[Epoch 23/25] [Batch 400/938] [D loss: 0.392144] [G loss: 1.340757]\n",
      "[Epoch 23/25] [Batch 500/938] [D loss: 0.595029] [G loss: 0.689880]\n",
      "[Epoch 23/25] [Batch 600/938] [D loss: 0.466516] [G loss: 1.089437]\n",
      "[Epoch 23/25] [Batch 700/938] [D loss: 0.745340] [G loss: 1.660226]\n",
      "[Epoch 23/25] [Batch 800/938] [D loss: 0.506967] [G loss: 1.623672]\n",
      "[Epoch 23/25] [Batch 900/938] [D loss: 0.471378] [G loss: 0.934058]\n",
      "[Epoch 24/25] [Batch 0/938] [D loss: 0.466179] [G loss: 0.867643]\n",
      "[Epoch 24/25] [Batch 100/938] [D loss: 0.563302] [G loss: 1.243455]\n",
      "[Epoch 24/25] [Batch 200/938] [D loss: 0.355621] [G loss: 1.545444]\n",
      "[Epoch 24/25] [Batch 300/938] [D loss: 0.198637] [G loss: 0.929912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/25] [Batch 400/938] [D loss: 0.468619] [G loss: 0.675721]\n",
      "[Epoch 24/25] [Batch 500/938] [D loss: 0.329600] [G loss: 0.785363]\n",
      "[Epoch 24/25] [Batch 600/938] [D loss: 0.261706] [G loss: 1.081682]\n",
      "[Epoch 24/25] [Batch 700/938] [D loss: 0.274637] [G loss: 1.414595]\n",
      "[Epoch 24/25] [Batch 800/938] [D loss: 0.472698] [G loss: 1.307553]\n",
      "[Epoch 24/25] [Batch 900/938] [D loss: 0.247356] [G loss: 1.705978]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "569.8033390045166"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('training start!')\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        #  Train Generator\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "    save_image(gen_imgs.data[:25], \"sample_images/%d.png\" % epoch, nrow=5, normalize=True)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for epoch in range(25):\n",
    "    img_name = 'sample_images/' + str(epoch) + '.png'\n",
    "    images.append(imageio.imread(img_name))\n",
    "imageio.mimsave('sample_images/animation.gif', images, fps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
